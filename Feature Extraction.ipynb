{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import deque\n",
    "from bert_serving.client import BertClient\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
    "from flair.data import Sentence\n",
    "import flair\n",
    "\n",
    "stacked_embeddings = WordEmbeddings('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacked_embeddings(sentence):\n",
    "    bert_embedding.embed(sentence)\n",
    "\n",
    "def tensorToNumpy(t):\n",
    "    return np.array(t[0].embedding.tolist())\n",
    "\n",
    "def get_embeds(sentences):\n",
    "    for i in sentences:\n",
    "        bert_embedding.embed(i)\n",
    "        \n",
    "def strip_punct_tensor(tensor):\n",
    "    return tensor.text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def strip_punct(instr):\n",
    "    return instr.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_as_a_service = '172.17.0.19'\n",
    "#bc = BertClient(ip=bert_as_a_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_vectors = [Sentence(i) for i in positive]\n",
    "for i in positive_vectors:\n",
    "    stacked_embeddings.embed(positive_vectors)\n",
    "negative_vectors = [Sentence(i) for i in negative]\n",
    "for i in positive_vectors:\n",
    "    stacked_embeddings.embed(negative_vectors)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = np.array([i[0].embedding.cpu().numpy() for i in positive_vectors + negative_vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0fa0d871d9ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_tokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpositive_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_tokenized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnegative_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bc' is not defined"
     ]
    }
   ],
   "source": [
    "vec = bc.encode([[i] for i in positive],is_tokenized=True,show_tokens=True)\n",
    "positive_vectors = np.array([vec[0][i][0] for i in range(len(positive))])\n",
    "vec = bc.encode([[i] for i in negative],is_tokenized=True,show_tokens=True)\n",
    "negative_vectors = np.array([vec[0][i][0] for i in range(len(negative))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=0, tol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train_vec\n",
    "Y = np.concatenate( ( np.ones(len(positive_vectors)), np.ones(len(negative_vectors))*-1 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=0, tol=1e-05,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8999557978488286"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(clf.predict(X_train_vec), Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get polarity scores for each of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = clf.coef_[0]\n",
    "def polarity(w, document_vector):\n",
    "    return np.dot(document_vector, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test\",'w') as f:\n",
    "    f.write(str(w.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_train_path = '../data401/reviews/train/pos/'\n",
    "neg_train_path = '../data401/reviews/train/neg/'\n",
    "\n",
    "pos_test_path = '../data401/reviews/test/pos/'\n",
    "neg_test_path = '../data401/reviews/test/neg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_of_interest = set(positive + negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity(document, filepath, words_of_interest, w):\n",
    "    \n",
    "    sentences = sent_tokenize(document)\n",
    "    \n",
    "    sentences_vec = [Sentence(i, use_tokenizer=True) for i in sentences]\n",
    "    \n",
    "    for sentence in sentences_vec:\n",
    "        stacked_embeddings.embed(sentence)\n",
    "    \n",
    "    words = []\n",
    "    for s_idx, sentence in enumerate(sentences_vec):\n",
    "        for w_idx, word in enumerate(sentence):\n",
    "            if word.text in words_of_interest:\n",
    "                words.append(sentences_vec[s_idx][w_idx].embedding)\n",
    "                \n",
    "    if (len(words) == 0):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return sum(words).cpu().numpy().dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pos_train_path+pos_train_files[0]) as f:\n",
    "    text = f.read().replace('<br />','\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1661118944414492"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_polarity(text, \"123\", words_of_interest, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair, torch\n",
    "\n",
    "flair.device = torch.device('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.71730637550354\n",
      "1000\n",
      "12.369462490081787\n",
      "2000\n",
      "13.241343259811401\n",
      "3000\n",
      "12.69118356704712\n",
      "4000\n",
      "12.801411867141724\n",
      "5000\n",
      "12.942528009414673\n",
      "6000\n",
      "12.6019606590271\n",
      "7000\n",
      "12.337921380996704\n",
      "8000\n",
      "12.999292373657227\n",
      "9000\n",
      "13.056675672531128\n",
      "10000\n",
      "12.951705694198608\n",
      "11000\n",
      "12.668541431427002\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "rslts = []\n",
    "import time\n",
    "start = time.time()\n",
    "measurement = 0\n",
    "for i in pos_train_files:\n",
    "    with open(pos_train_path+i) as f:\n",
    "        measurement+= 1\n",
    "        text = f.read().replace('<br />','\\n')\n",
    "        rslts.append((i, get_polarity(text, pos_train_path+i, words_of_interest, w)))\n",
    "        if (measurement % 1000 == 0):\n",
    "            end = time.time()\n",
    "            print(end - start)\n",
    "            start = end\n",
    "            print(measurement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(rslts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.to_csv(\"pos_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "files = []\n",
    "for i in pos_train_files:\n",
    "    with open(pos_train_path+i) as f:\n",
    "        text = f.read().replace('<br />','\\n')\n",
    "        files.append((pos_train_path+i, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polarity(sentence_vectors, tokens, filepath, w):\n",
    "    \n",
    "    rslt = []        \n",
    "    for s_idx, sentence in enumerate(tokens):\n",
    "        for w_idx, word in enumerate(sentence):\n",
    "            if word in words_of_interest:\n",
    "                rslt.append(sentence_vectors[s_idx][w_idx])\n",
    "                \n",
    "    if (len(rslt) == 0):\n",
    "        pol = np.nan\n",
    "    else:\n",
    "        pol = sum(rslt).dot(w)   \n",
    "        \n",
    "    return ({'filepath':filepath, 'polarity': pol})\n",
    "    \n",
    "    \n",
    "def get_polarities(base_path, filenames, w):\n",
    "    \n",
    "    rslts = []\n",
    "    iterations = 0\n",
    "    \n",
    "    sentences = []\n",
    "\n",
    "    sentence_mapping = []\n",
    "\n",
    "    print(\"Building sentences\")\n",
    "    for filename in filenames:\n",
    "    \n",
    "        with open(base_path+filename) as f:\n",
    "        \n",
    "            text = f.read().replace('<br />','\\n')\n",
    "            input_sentence = sent_tokenize(text)\n",
    "        \n",
    "            start = len(sentences)\n",
    "            end = start + len(input_sentence)\n",
    "        \n",
    "            sentences = sentences + input_sentence\n",
    "            sentence_mapping.append((start, end))\n",
    "    \n",
    "    print(\"sentences built\")\n",
    "        \n",
    "    index = 0\n",
    "    current_batch_start = 0\n",
    "    current_batch_end = 0\n",
    "    queue = []\n",
    "    for i,(j,k) in enumerate(sentence_mapping):\n",
    "        if (current_batch_end - current_batch_start < 37000):\n",
    "            current_batch_end = k\n",
    "            queue.append((i,(j,k)))\n",
    "        else:\n",
    "            print(\"sending ~37k batched: Current at \" + str(i) +\" files...\")\n",
    "            (sentence_vectors, tokens) = bc.encode(sentences[current_batch_start:current_batch_end], show_tokens=True)\n",
    "            for (ni,(ji,ki)) in queue:\n",
    "                jj = ji - current_batch_start\n",
    "                kj = ki - current_batch_start\n",
    "                rslts.append(get_polarity(sentence_vectors[jj:kj], tokens[jj:kj], base_path+filenames[ni], w))\n",
    "            queue = []\n",
    "            current_batch_start = current_batch_end\n",
    "            \n",
    "    if (len(queue) > 0):\n",
    "        print(\"cleaning up\")\n",
    "        (sentence_vectors, tokens) = bc.encode(sentences[current_batch_start:], show_tokens=True)\n",
    "        for (ni,(ji,ki)) in queue:\n",
    "            jj = ji - current_batch_start\n",
    "            kj = ki - current_batch_start\n",
    "            rslts.append(get_polarity(sentence_vectors[jj:kj], tokens[jj:kj], base_path+filenames[ni], w))\n",
    "        queue = []\n",
    "        current_batch_start = current_batch_end\n",
    "        \n",
    "    return rslts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making positive_train\n",
      "Building sentences\n",
      "sentences built\n",
      "sending ~37k batched: Current at 3045 files...\n",
      "sending ~37k batched: Current at 6081 files...\n",
      "sending ~37k batched: Current at 9065 files...\n",
      "sending ~37k batched: Current at 12171 files...\n",
      "cleaning up\n"
     ]
    }
   ],
   "source": [
    "print(\"making positive_train\")\n",
    "pos_train = get_polarities(pos_train_path, pos_train_files, w)\n",
    "pd.DataFrame(pos_train).to_csv(\"pos_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making negative_train\n",
      "Building sentences\n",
      "sentences built\n",
      "sending ~37k batched: Current at 2906 files...\n",
      "sending ~37k batched: Current at 5828 files...\n",
      "sending ~37k batched: Current at 8707 files...\n",
      "sending ~37k batched: Current at 11629 files...\n",
      "cleaning up\n"
     ]
    }
   ],
   "source": [
    "print(\"making negative_train\")\n",
    "\n",
    "neg_train = get_polarities(neg_train_path, neg_train_files, w)\n",
    "pd.DataFrame(neg_train).to_csv(\"neg_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making positive_test\n",
      "Building sentences\n",
      "sentences built\n",
      "sending ~37k batched: Current at 3108 files...\n",
      "sending ~37k batched: Current at 6153 files...\n",
      "sending ~37k batched: Current at 9302 files...\n",
      "sending ~37k batched: Current at 12486 files...\n",
      "cleaning up\n"
     ]
    }
   ],
   "source": [
    "print(\"making positive_test\")\n",
    "\n",
    "pos_test =  get_polarities(pos_test_path,  pos_test_files, w)\n",
    "pd.DataFrame(pos_test).to_csv(\"pos_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making negative_test\n",
      "Building sentences\n",
      "sentences built\n",
      "sending ~37k batched: Current at 2952 files...\n",
      "sending ~37k batched: Current at 5898 files...\n",
      "sending ~37k batched: Current at 8834 files...\n",
      "sending ~37k batched: Current at 11792 files...\n",
      "cleaning up\n"
     ]
    }
   ],
   "source": [
    "print(\"making negative_test\")\n",
    "\n",
    "neg_test =  get_polarities(neg_test_path,  neg_test_files, w)\n",
    "pd.DataFrame(neg_test).to_csv(\"neg_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data401/reviews/imdb.vocab') as f:\n",
    "    vocab = pd.Series(f.readlines())[:10000].apply(lambda x: x.rstrip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
