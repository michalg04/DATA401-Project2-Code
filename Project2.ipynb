{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "### Goal: Build classifiers\n",
    "- Logistic Regression\n",
    "- Linear Discriminant Analysis\n",
    "- Support Vector Machines\n",
    "\n",
    "### Project Notes\n",
    "- Always train on ‘train/’ directory files and test on ‘test/’ directory files.\n",
    "- Should be capturing sentiment, not information about the movie itself.\n",
    "- No external data related to movies, can use other external data related to sentiment analysis (e.g. sentiment scores of different words).\n",
    "- Vectorization of the text (TFIDF, bigrams, etc.) - can use packages, but may need to go above and beyond what the packages do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py-readability-metrics in /usr/local/lib/python3.6/dist-packages (1.3.5)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from py-readability-metrics) (3.4.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->py-readability-metrics) (1.12.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.4.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install py-readability-metrics\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Functionality Imported from Utilities.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from readability import Readability\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "from project2 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1:\n",
    "You will construct your own feature set by analyzing the textual data and vectorizing it as you see fit.\n",
    "\n",
    "Your project involves a competition between the existing feature set provided to you as part of the dataset and a feature set that you develop yourselves.\n",
    "\n",
    "When setting up your vectorization process, please note the following facts. The training set has no more than 30 reviews per individual movie. The movies in the test set are different than the movies in the training set. Therefore, this assignment really is about sentiment detection from the text, not about determining the relationship between movies and their ratings. Therefore, unlike Project 1, where your goal was to bring in features from outside datasets, for Project 2, your feature extraction shall concentrate solely on the text of the reviews. \n",
    "You can use outside datasets related to language (e.g., datasets that specify word sentiment), but do not use any outside sources of information about the movies themselves. Your review models shall largely be agnostic of the specific movies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Train and Test Reviews into DataFrames\n",
    "**Variables:**\n",
    "   - filename: full path to the file the review is found in \n",
    "   - text: enitre text of the review\n",
    "   - sentiment: 1 for positive, 0 for negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train():\n",
    "    print(\"\\nGetting positive reviews...\\n\")\n",
    "    pos_train_path = '/data401/reviews/train/pos/'\n",
    "    pos_train = []\n",
    "    i = 0\n",
    "    start = datetime.datetime.now()\n",
    "    for filename in pos_train_files:\n",
    "        with open(pos_train_path + filename) as f:\n",
    "            text = f.read().replace('<br />','\\n')\n",
    "        pos_train.append({\n",
    "            'filename': pos_train_path + filename,\n",
    "            'text': text\n",
    "        })\n",
    "\n",
    "        if i%1000 == 0:\n",
    "            print(i, datetime.datetime.now() - start)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    pos_train_df = pd.DataFrame(pos_train)\n",
    "    pos_train_df['sentiment'] = 1\n",
    "    \n",
    "    print(\"\\nGetting negative reviews...\\n\")\n",
    "    neg_train_path = '/data401/reviews/train/neg/'\n",
    "    neg_train = []\n",
    "    i = 0\n",
    "    start = datetime.datetime.now()\n",
    "    for filename in neg_train_files:\n",
    "        with open(neg_train_path + filename) as f:\n",
    "            text = f.read().replace('<br />','\\n')\n",
    "        neg_train.append({\n",
    "            'filename': neg_train_path + filename,\n",
    "            'text': text\n",
    "        })\n",
    "\n",
    "        if i%1000 == 0:\n",
    "            print(i, datetime.datetime.now() - start)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    neg_train_df = pd.DataFrame(neg_train)\n",
    "    neg_train_df['sentiment'] = 0\n",
    "    \n",
    "    print('\\nCombining DataFrames\\n')\n",
    "    train_df = pd.concat([pos_train_df, neg_train_df], sort = False).fillna(0)\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def get_test():\n",
    "    print(\"\\nGetting positive reviews...\\n\")\n",
    "    pos_test_path = '/data401/reviews/test/pos/'\n",
    "    pos_test = []\n",
    "    i = 0\n",
    "    start = datetime.datetime.now()\n",
    "    for filename in pos_test_files:\n",
    "        with open(pos_test_path + filename) as f:\n",
    "            text = f.read().replace('<br />','\\n')\n",
    "        pos_test.append({\n",
    "            'filename': pos_test_path + filename,\n",
    "            'text': text\n",
    "        })\n",
    "\n",
    "        if i%1000 == 0:\n",
    "            print(i, datetime.datetime.now() - start)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    pos_test_df = pd.DataFrame(pos_test)\n",
    "    pos_test_df['sentiment'] = 1\n",
    "    \n",
    "    print(\"\\nGetting negative reviews...\\n\")\n",
    "    neg_test_path = '/data401/reviews/test/neg/'\n",
    "    neg_test = []\n",
    "    i = 0\n",
    "    start = datetime.datetime.now()\n",
    "    for filename in neg_test_files:\n",
    "        with open(neg_test_path + filename) as f:\n",
    "            text = f.read().replace('<br />','\\n')\n",
    "        \n",
    "        neg_test.append({\n",
    "            'filename': neg_test_path+filename,\n",
    "            'text': text\n",
    "        })\n",
    "\n",
    "        if i%1000 == 0:\n",
    "            print(i, datetime.datetime.now() - start)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    neg_test_df = pd.DataFrame(neg_test)\n",
    "    neg_test_df['sentiment'] = 0\n",
    "    \n",
    "    print('\\nCombining DataFrames\\n')\n",
    "    test_df = pd.concat([pos_test_df, neg_test_df], sort = False).fillna(0)\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train.csv' not in os.listdir('project2_data'):\n",
    "    # This takes about 6 minutes\n",
    "    train_df = get_train()\n",
    "    train_df.to_csv('project2_data/train.csv', index = False)\n",
    "else:\n",
    "    train_df = pd.read_csv('project2_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'test.csv' not in os.listdir('project2_data'):\n",
    "    # This takes about 6 minutes\n",
    "    test_df = get_test()\n",
    "    test_df.to_csv('project2_data/test.csv', index = False)\n",
    "else:\n",
    "    test_df = pd.read_csv('project2_data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Features\n",
    "**Basic Variables**\n",
    "- numer of sentences\n",
    "- average numer of words per sentence\n",
    "- average word length\n",
    "\n",
    "**Readability / Complexity**\n",
    "- dale chall readability score\n",
    "\n",
    "**Sentiment**\n",
    "\n",
    "Looking at:\n",
    "- positive / negative emoji use\n",
    "- positive / negative word use\n",
    "- boosters and diminishers\n",
    "\n",
    "Variables:\n",
    "- positive emoticons: number of positive emoticons adjusted for number of words\n",
    "- negative emoticons: number of negative emoticons adjusted for number of words\n",
    "- positive words: number of positive words adjusted for number of words, weighted by looking at boosting and diminishing words in the same sentence.\n",
    "- negative words: number of negative words adjusted for number of words, weighted by looking at boosting and diminishing words in the same sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_readability(text):\n",
    "    try:\n",
    "        return Readability(text).dale_chall().score\n",
    "    except Exception:\n",
    "        # Readability requires 100 words. If there aren't enough,\n",
    "        # concatenate the text to itself and try again.\n",
    "        text = text + ' ' + text\n",
    "        return calculate_readability(text)\n",
    "    \n",
    "def extract_emoticons(text, num_words):\n",
    "    added_features = {\n",
    "        'positive_emoticons': 0,\n",
    "        'negative_emoticons': 0\n",
    "    }\n",
    "    \n",
    "    for emoticon in positive_emoticons:\n",
    "        if emoticon in text:\n",
    "            added_features['positive_emoticons'] +=  1/num_words\n",
    "            \n",
    "    for emoticon in negative_emoticons:\n",
    "        if emoticon in text:\n",
    "            added_features['negative_emoticons'] +=  1/num_words\n",
    "            \n",
    "    return added_features\n",
    "\n",
    "def get_weight(sentence):\n",
    "    # Checking for boosters and diminishers in a sentence\n",
    "    # Baseline weight is 1\n",
    "    # If net boosting: 1.5\n",
    "    # If net diminishing: 0.5\n",
    "    \n",
    "    net = 0\n",
    "    for dim in diminisher_words:\n",
    "        if dim in sentence:\n",
    "            net -= 1\n",
    "    for boost in booster_words:\n",
    "        if boost in sentence:\n",
    "            net += 1\n",
    "         \n",
    "    negated = False\n",
    "    for neg in negation_words:\n",
    "        if neg in sentence:\n",
    "            negated = True\n",
    "            \n",
    "    if net == 0:\n",
    "        w = 1\n",
    "    elif net > 0:\n",
    "        w = 1.5\n",
    "    else:\n",
    "        w = 0.5\n",
    "        \n",
    "    return w, negated\n",
    "    \n",
    "\n",
    "def extract_words(sentences, num_words):\n",
    "    added_features = {\n",
    "        'positive_words': 0,\n",
    "        'negative_words': 0\n",
    "    }\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        weight, negated = get_weight(sentence)\n",
    "\n",
    "        for word in words:\n",
    "            if True:\n",
    "#             if not negated:\n",
    "                if word in positive:\n",
    "                    added_features['positive_words'] += weight/num_words\n",
    "                if word in negative:\n",
    "                    added_features['negative_words'] += weight/num_words\n",
    "#             else:\n",
    "#                 if word in positive:\n",
    "#                     added_features['negative_words'] += weight/num_words\n",
    "#                 if word in negative:\n",
    "#                     added_features['positive_words'] += weight/num_words\n",
    "    \n",
    "    return added_features\n",
    "    \n",
    "def get_features(text, filename):\n",
    "    features = {\n",
    "        'filename': filename\n",
    "    }\n",
    "       \n",
    "    # Basic Variables\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    features['num_sentences'] = len(sentences)\n",
    "    features['words_per_senence'] = len(words) / len(sentences)\n",
    "\n",
    "    word_lengths = np.array([len(w) for w in words])\n",
    "    features['avg_word_length'] = word_lengths.mean()\n",
    "    \n",
    "    # Readability / Complexity\n",
    "    features['dale_chall_readability'] = calculate_readability(text)\n",
    "    \n",
    "    # Sentiment\n",
    "    features.update(extract_emoticons(text, len(words)))\n",
    "    features.update(extract_words(sentences, len(words)))\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test feature extraction on one observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': '/data401/reviews/train/pos/2655_10.txt',\n",
       " 'num_sentences': 17,\n",
       " 'words_per_senence': 51.294117647058826,\n",
       " 'avg_word_length': 4.283256880733945,\n",
       " 'dale_chall_readability': 10.530646269822672,\n",
       " 'positive_emoticons': 0,\n",
       " 'negative_emoticons': 0,\n",
       " 'positive_words': 0.046444954128440394,\n",
       " 'negative_words': 0.04300458715596331}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_features(train_df['text'][0], train_df['filename'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply feature extraction to test and train dataframes. Save new data frames to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func, n_cores=8):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def get_features_parallel(df):\n",
    "    trained_features = []\n",
    "    for i, row in df.iterrows():\n",
    "        trained_features.append(get_features(row['text'], row['filename']))\n",
    "    return pd.DataFrame(trained_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'train_features.csv' not in os.listdir('project2_data'):\n",
    "    train_features = parallelize_dataframe(train_df, get_features_parallel)\n",
    "    \n",
    "    train_features_df = pd.DataFrame(train_features).reset_index(drop=True)\n",
    "    train_features_df.to_csv('project2_data/train_features.csv', index = False)\n",
    "else:\n",
    "    train_features_df = pd.read_csv('project2_data/train_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'test_features.csv' not in os.listdir('project2_data'):\n",
    "    test_features = parallelize_dataframe(test_df, get_features_parallel)\n",
    "\n",
    "    test_features_df = pd.DataFrame(test_features).reset_index(drop=True)\n",
    "    test_features_df.to_csv('project2_data/test_features.csv', index = False)\n",
    "else:\n",
    "    test_features_df = pd.read_csv('project2_data/test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>words_per_senence</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>dale_chall_readability</th>\n",
       "      <th>positive_emoticons</th>\n",
       "      <th>negative_emoticons</th>\n",
       "      <th>positive_words</th>\n",
       "      <th>negative_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>/data401/reviews/test/pos/2655_10.txt</td>\n",
       "      <td>23</td>\n",
       "      <td>30.173913</td>\n",
       "      <td>4.187320</td>\n",
       "      <td>9.801523</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.048991</td>\n",
       "      <td>0.033141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>/data401/reviews/test/pos/4521_7.txt</td>\n",
       "      <td>10</td>\n",
       "      <td>21.900000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.001824</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043379</td>\n",
       "      <td>0.054795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>/data401/reviews/test/pos/12429_10.txt</td>\n",
       "      <td>7</td>\n",
       "      <td>25.714286</td>\n",
       "      <td>3.861111</td>\n",
       "      <td>7.975813</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>/data401/reviews/test/pos/3384_10.txt</td>\n",
       "      <td>13</td>\n",
       "      <td>36.230769</td>\n",
       "      <td>4.214437</td>\n",
       "      <td>9.042558</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045648</td>\n",
       "      <td>0.016985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>/data401/reviews/test/pos/6697_7.txt</td>\n",
       "      <td>9</td>\n",
       "      <td>15.777778</td>\n",
       "      <td>3.887324</td>\n",
       "      <td>8.609708</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.066901</td>\n",
       "      <td>0.080986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 filename  num_sentences  words_per_senence  \\\n",
       "0   /data401/reviews/test/pos/2655_10.txt             23          30.173913   \n",
       "1    /data401/reviews/test/pos/4521_7.txt             10          21.900000   \n",
       "2  /data401/reviews/test/pos/12429_10.txt              7          25.714286   \n",
       "3   /data401/reviews/test/pos/3384_10.txt             13          36.230769   \n",
       "4    /data401/reviews/test/pos/6697_7.txt              9          15.777778   \n",
       "\n",
       "   avg_word_length  dale_chall_readability  positive_emoticons  \\\n",
       "0         4.187320                9.801523                   0   \n",
       "1         4.000000                9.001824                   0   \n",
       "2         3.861111                7.975813                   0   \n",
       "3         4.214437                9.042558                   0   \n",
       "4         3.887324                8.609708                   0   \n",
       "\n",
       "   negative_emoticons  positive_words  negative_words  \n",
       "0                   0        0.048991        0.033141  \n",
       "1                   0        0.043379        0.054795  \n",
       "2                   0        0.044444        0.083333  \n",
       "3                   0        0.045648        0.016985  \n",
       "4                   0        0.066901        0.080986  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesofinterest = ['filename','polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_test_polarity = pd.read_csv('project2_data/polarity/neg_test.csv')\n",
    "pos_test_polarity = pd.read_csv('project2_data/polarity/pos_test.csv')\n",
    "\n",
    "neg_test_polarity = neg_test_polarity[filesofinterest]\n",
    "neg_test_polarity['filename'] = neg_test_polarity['filename'].apply(lambda x: \"/data401/reviews/test/neg/\"+x)\n",
    "\n",
    "pos_test_polarity = pos_test_polarity[filesofinterest]\n",
    "pos_test_polarity['filename'] = pos_test_polarity['filename'].apply(lambda x: \"/data401/reviews/test/pos/\"+x)\n",
    "\n",
    "test_polarity = pd.concat([neg_test_polarity, pos_test_polarity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train_polarity = pd.read_csv('project2_data/polarity/neg_train.csv')\n",
    "pos_train_polarity = pd.read_csv('project2_data/polarity/pos_train.csv')\n",
    "\n",
    "neg_train_polarity = neg_train_polarity[filesofinterest]\n",
    "neg_train_polarity['filename'] = neg_train_polarity['filename'].apply(lambda x: \"/data401/reviews/train/neg/\"+x)\n",
    "\n",
    "pos_train_polarity = pos_train_polarity[filesofinterest]\n",
    "pos_train_polarity['filename'] = pos_train_polarity['filename'].apply(lambda x: \"/data401/reviews/train/pos/\"+x)\n",
    "\n",
    "train_polarity = pd.concat([neg_train_polarity, pos_train_polarity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score = pd.concat([test_polarity, train_polarity])['polarity'].mean()\n",
    "train_polarity = train_polarity.fillna(mean_score)\n",
    "test_polarity = test_polarity.fillna(mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_df = test_features_df.merge(test_polarity, on='filename')\n",
    "train_features_df = train_features_df.merge(train_polarity, on='filename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2:\n",
    "You will use your classifier implementations to fit models for classifying movie reviews into positive and negative both on the feature set provided to you and on the feature set you created.\n",
    "\n",
    "Implement three linear classifiers we discussed in class: Logistic Regression, Linear Discriminant Analysis, and Support Vector Machines. Use gradient descent/stochastic gradient descent for Logistic Regression and SVM classifiers. For Linear Discriminant Analysis use NumPy’s methods for discovering eigenvalues and eigenvectors of a matrix.\n",
    "For this project you can limit your implementations to two-class classifiers, as Stanford’s Large Movie Review dataset has two classes.\n",
    "\n",
    "For this project you will train the classifiers on the training set provided to you and evaluate them on the test set. Because of how both the training and the test set are constructed, do not use cross-validation, or other evaluation techniques, as they might produce skewed results.\n",
    "As both positive and negative reviews are balanced in the test and training sets, and both are equally important to detect, your key measure is accuracy.  In addition, the software you build shall produce confusion matrices to give you an idea of what errors are more prevalent, and allow you to tune your classifier models.\n",
    "Where your method comes with parameters, use grid search to hyper tune them. Do not consider learning rate a parameter of the model for methods where gradient descent is involved. Find the appropriate learning rate for each classification task, though.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = train_features_df.merge(\n",
    "    train_df[['filename','sentiment']], \n",
    "    on = 'filename')\n",
    "X_train = Train.drop(\n",
    "    columns = ['sentiment']\n",
    ")\n",
    "y_train = Train['sentiment']\n",
    "\n",
    "Test = test_features_df.merge(\n",
    "    test_df[['filename','sentiment']],\n",
    "    on = 'filename'\n",
    ")\n",
    "X_test = Test.drop(\n",
    "    columns = ['sentiment']\n",
    ")\n",
    "y_test = Test['sentiment']\n",
    "\n",
    "X_train = X_train.drop(columns = ['filename']).fillna(mean_score)\n",
    "X_test = X_test.drop(columns = ['filename']).fillna(mean_score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = X_train.columns.tolist()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = X_train.append(X_test).copy()\n",
    "combined[col_names] = scaler.fit_transform(combined[col_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = combined.head(25000)\n",
    "X_test = combined.tail(25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74828\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(solver = 'sag')\n",
    "clf.fit(X_train, y_train) \n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy = sum(predictions == np.array(y_test))/len(predictions)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 0.75068\n",
      "Test Accuracy = 0.74668\n"
     ]
    }
   ],
   "source": [
    "model = fitLogistic(X_train, y_train, rate = 0.01, tol = 1, maxiter = 1000)\n",
    "train_predictions = classifyLogistic(X_train, model)\n",
    "train_accuracy = (train_predictions == y_train).sum()/len(train_predictions)\n",
    "print(\"Train Accuracy =\",train_accuracy)\n",
    "\n",
    "test_predictions = classifyLogistic(X_test, model)\n",
    "test_accuracy = (test_predictions == y_test).sum()/len(test_predictions)\n",
    "print(\"Test Accuracy =\",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis\n",
    "Implementation Using Eigenvalues / Eigenvectors in [the python script](project2.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 0.75104\n",
      "Test Accuracy = 0.74856\n"
     ]
    }
   ],
   "source": [
    "w, mu0, mu1 = fitLDA(X_train, y_train)\n",
    "train_predictions = classifyLDA(X_train, w, mu0, mu1)\n",
    "train_accuracy = (train_predictions == y_train).sum()/len(train_predictions)\n",
    "print(\"Train Accuracy =\",train_accuracy)\n",
    "\n",
    "test_predictions = classifyLDA(X_test, w, mu0, mu1)\n",
    "test_accuracy = (test_predictions == y_test).sum()/len(test_predictions)\n",
    "print(\"Test Accuracy =\",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "Using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74732\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=1, tol=1, max_iter = 100000, kernel = 'linear')\n",
    "clf.fit(X_train, y_train) \n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "accuracy = sum(predictions == np.array(y_test))/len(predictions)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.696\n"
     ]
    }
   ],
   "source": [
    "w = fitSVM(X_train,y_train,.01,.0000001,.4)\n",
    "svm_out = predictSVM(X_test.values, w)\n",
    "accuracy = accuracy_score(y_test, svm_out)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:\n",
    "You will compare the performance of the three classifiers to each other on each of the feature sets.\n",
    "\n",
    "Question 2. How do the three classification techniques you implemented compare on the Large Movie Reviews dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that our initial feature set still performs worse when compared to the baseline, but what if we combine the two datasets together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data401/reviews/imdb.vocab') as f:\n",
    "    vocab = [l.replace('\\n','') for l in f.readlines()]\n",
    "\n",
    "X_test_other, y_test_other = load_svmlight_file(\n",
    "    '../data401/reviews/test/labeledBow.feat',\n",
    "    n_features = len(vocab)\n",
    ")\n",
    "y_test_other = np.array([-1 if y <=5 else 1 for y in y_test_other])\n",
    "y_test_other_01 = np.array([0 if y == -1 else 1 for y in y_test_other])\n",
    "\n",
    "X_train_other, y_train_other = load_svmlight_file(\n",
    "    '../data401/reviews/train/labeledBow.feat',\n",
    "    n_features = len(vocab)\n",
    ")\n",
    "y_train_other = np.array([-1 if y <=5 else 1 for y in y_train_other])\n",
    "y_train_other_01 = np.array([0 if y == -1 else 1 for y in y_train_other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s = pd.DataFrame(X_train_other.todense()).iloc[:,:500]\n",
    "X_test_s = pd.DataFrame(X_test_other.todense()).iloc[:,:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combined = pd.concat([X_train, X_train_s],axis=1,sort=False)\n",
    "X_test_combined = pd.concat([X_test, X_test_s],axis=1,sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic top 500 and Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver = 'sag')\n",
    "clf.fit(X_train_s, y_train) \n",
    "\n",
    "predictions = clf.predict(X_test_s)\n",
    "accuracy = sum(predictions == np.array(y_test))/len(predictions)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver = 'sag')\n",
    "clf.fit(X_train_combined, y_train) \n",
    "\n",
    "predictions = clf.predict(X_test_combined)\n",
    "accuracy = sum(predictions == np.array(y_test))/len(predictions)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA combined and top 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 0.87928\n",
      "Test Accuracy = 0.87336\n"
     ]
    }
   ],
   "source": [
    "w, mu0, mu1 = fitLDA(X_train_combined, y_train)\n",
    "train_predictions = classifyLDA(X_train_combined, w, mu0, mu1)\n",
    "train_accuracy = (train_predictions == y_train).sum()/len(train_predictions)\n",
    "print(\"Train Accuracy =\",train_accuracy)\n",
    "\n",
    "test_predictions = classifyLDA(X_test_combined, w, mu0, mu1)\n",
    "test_accuracy = (test_predictions == y_test).sum()/len(test_predictions)\n",
    "print(\"Test Accuracy =\",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy = 0.84644\n",
      "Test Accuracy = 0.8386\n"
     ]
    }
   ],
   "source": [
    "w, mu0, mu1 = fitLDA(X_train_s, y_train)\n",
    "train_predictions = classifyLDA(X_train_s, w, mu0, mu1)\n",
    "train_accuracy = (train_predictions == y_train).sum()/len(train_predictions)\n",
    "print(\"Train Accuracy =\",train_accuracy)\n",
    "\n",
    "test_predictions = classifyLDA(X_test_s, w, mu0, mu1)\n",
    "test_accuracy = (test_predictions == y_test).sum()/len(test_predictions)\n",
    "print(\"Test Accuracy =\",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM top 500 and combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(C=1, tol=1, max_iter = 10000)\n",
    "clf.fit(X_train_s, y_train) \n",
    "\n",
    "predictions = clf.predict(X_test_s)\n",
    "accuracy = sum(predictions == np.array(y_test))/len(predictions)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC(C=1, tol=1, max_iter = 10000)\n",
    "clf.fit(X_train_combined, y_train) \n",
    "\n",
    "predictions = clf.predict(X_test_combined)\n",
    "accuracy = sum(predictions == np.array(y_test))/len(predictions)\n",
    "\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
